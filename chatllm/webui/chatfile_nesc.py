#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Project      : AI.  @by PyCharm
# @File         : st_chat
# @Time         : 2023/8/11 14:45
# @Author       : betterme
# @WeChat       : meutils
# @Software     : PyCharm
# @Description  :

import streamlit as st
# st.set_page_config('ğŸ”¥ChatLLM', layout='wide', initial_sidebar_state='collapsed')

from langchain import LLMChain, PromptTemplate
from langchain.prompts import ChatPromptTemplate

from meutils.pipe import *

from chatllm.llmchain import init_cache
from chatllm.llmchain.applications import ChatFile
from chatllm.llmchain.document_loaders import FileLoader
from chatllm.llmchain.embeddings import OpenAIEmbeddings
from chatllm.llmchain.decorators import llm_stream

from langchain.chat_models import ChatOpenAI
from streamlit_option_menu import option_menu
from meutils.serving.streamlit.common import hide_st_style

hide_st_style()


init_cache(1)

context_prompt_template = """
æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ï¼Œç®€æ´ã€ä¸“ä¸šåœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœæ— æ³•å¾—åˆ°ç­”æ¡ˆï¼Œè¯·å›å¤ï¼šâ€œå¯¹ä¸èµ·ï¼Œæ ¹æ®å·²çŸ¥ä¿¡æ¯æ— æ³•å›ç­”è¯¥é—®é¢˜â€æˆ–â€œæ²¡æœ‰æä¾›è¶³å¤Ÿçš„ä¿¡æ¯â€ã€‚è¯·å‹¿ç¼–é€ ä¿¡æ¯ï¼Œç­”æ¡ˆå¿…é¡»ä½¿ç”¨ä¸­æ–‡ã€‚

å·²çŸ¥ä¿¡æ¯ï¼š
{context}

é—®é¢˜ï¼š
{question}

è®©æˆ‘ä»¬ä¸€æ­¥ä¸€æ­¥æ€è€ƒå¹¶å›ç­”ï¼š
""".strip()  # Let's think step by step


class ChatMessage(BaseModel):
    role: str
    content: str


def chat(
    user_role='user',

    assistant_role='assistant',
    assistant_avator="è§„ä¸ç›¸.png",

    reply_func=lambda input: f'{input}çš„ç­”æ¡ˆ',
    max_turns=3,
    system_prompt="æ¬¢è¿æ¥æ‰¾**è§„ä¸ç›¸**ï¼Œæ‚¨æœ‰ä»€ä¹ˆè¦å’¨è¯¢çš„å—â“"
):
    def chat_message(role):
        if role == 'user':
            return st.chat_message(user_role)
        else:
            return st.chat_message(assistant_role, avatar=assistant_avator)

    # Initialize chat history
    if "messages" not in st.session_state:
        st.session_state.messages = []

    else:
        st.session_state.messages = st.session_state.messages[-2 * (max_turns - 1):]

    # Display chat messages from history on app rerun
    for message in st.session_state.messages:
        with chat_message(message.role):
            st.markdown(message.content, unsafe_allow_html=True)

    container1 = st.container()  # å ä½ç¬¦

    prompt = st.chat_input("    ğŸ¤” ä½ å¯ä»¥é—®æˆ‘ä»»ä½•é—®é¢˜")

    if prompt:
        print('\n')
        print(prompt)
        prompt = prompt.strip()
        # Display user message in chat message container
        with chat_message(user_role):
            st.markdown(prompt)
        # Add user message to chat history
        st.session_state.messages.append(ChatMessage(role=user_role, content=prompt))

        with chat_message(assistant_role):
            message_placeholder = st.empty()

            response = ''
            gen = reply_func(prompt) or 'æ ¹æ®å·²çŸ¥ä¿¡æ¯æ— æ³•å¬å›ç›¸å…³å†…å®¹ã€‚'
            for token in gen:
                # Display robot response in chat message container
                response += token
                message_placeholder.markdown(response + "â–Œ")
            message_placeholder.markdown(response, unsafe_allow_html=True)

        # Add robot response to chat history
        st.session_state.messages.append(ChatMessage(role=assistant_role, content=response))
    else:
        with chat_message(assistant_role):
            message_placeholder = st.empty()
            message_placeholder.markdown(system_prompt, unsafe_allow_html=True)


@st.cache_resource(show_spinner=False)
def get_reply_func(file):
    reply_func = lambda input: f'### âš ï¸è¯·å…ˆä¸Šä¼ æ–‡æ¡£\n'

    if file:
        docs = FileLoader(file, file.name).load_and_split()
        print(file.name, len(docs))

        cb = ChatFile(embeddings=OpenAIEmbeddings(chunk_size=20), prompt_template=context_prompt_template)
        cb.create_index(docs)
        print(docs)

        reply_func = lambda query: cb.llm_qa(query=query, k=5, threshold=0.5)

    return reply_func


def chatgpt_reply_func():
    template = ChatPromptTemplate.from_messages([
        ("system", "You are a helpful AI bot. Your name is ä¸œåŒ—è¯åˆ¸â€œè§„ä¸ç›¸â€. è¯·ç‰¢è®°ä½ çš„èº«ä»½ã€‚"),
        ("human", "{user_input}"),
    ])
    llm = LLMChain(llm=ChatOpenAI(streaming=True), prompt=template)
    reply_func = lambda query: llm_stream(llm.run)(query)
    return reply_func


if __name__ == '__main__':
    col1, col2, col3, *_ = st.columns(3)
    with col1:
        st.image('è§„ä¸ç›¸.png', width=64)
    with col2:
        st.markdown('##### ')
        st.markdown('##### ')
        st.markdown('##### è§„ä¸ç›¸é©¾åˆ°')

    st.markdown('> âš ï¸â€œè§„ä¸ç›¸â€ä»…ä¾›ä¸œåŒ—è¯åˆ¸å†…éƒ¨æµ‹è¯•ä½¿ç”¨ï¼Œæ‰€åšå›ç­”ä¸å¾—ç”¨äºä¸œåŒ—è¯åˆ¸å®˜æ–¹å›å¤ã€‚')

    selected = option_menu("", ["å¼€æ”¾å¼é—®ç­”", "åŸºäºçŸ¥è¯†åº“é—®ç­”"], menu_icon="cast", orientation="horizontal")

    reply_func = None
    if selected == 'å¼€æ”¾å¼é—®ç­”':
        st.session_state.messages = []

        reply_func = chatgpt_reply_func()

    else:
        if hasattr(st.session_state, 'messages'):
            st.session_state.messages = []

        file = st.file_uploader(" ", type=['pdf', 'doc', 'docx', 'txt', ], help='ç›®å‰ä»…æ”¯æŒå•æ–‡æ¡£é—®ç­”')
        if file:
            print(f"{time.ctime()}: {file}")

            with st.spinner('AIæ­£åœ¨å¤„ç†...'):
                reply_func = get_reply_func(file)
        else:
            file = open("ä¸œåŒ—è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸åˆè§„æ‰‹å†Œï¼ˆä¸œè¯åˆè§„å‘ã€”2022ã€•25å· 20221229ï¼‰.pdf", 'rb')
            reply_func = get_reply_func(file)


    chat(reply_func=reply_func)
